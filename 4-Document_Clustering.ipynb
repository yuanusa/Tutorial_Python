{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Text Data\n",
    "\n",
    "BitTiger DS501"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment uses 'articles.pkl' file that has 1405 articles from 'Arts','Books','Business Day', 'Magazine', 'Opinion', 'Real Estate', 'Sports', 'Travel', 'U.S.', and 'World'. This is a [pickled](https://docs.python.org/2/library/pickle.html) data frame and can be loaded back into a [data frame](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_pickle.html#pandas.read_pickle).  You probably want to eventually get it out of pandas DataFrames when you perform your analysis.\n",
    "\n",
    "| Section | count|\n",
    "| :---| :--|\n",
    "|Arts| 91|\n",
    "|Automobiles| 5|\n",
    "|Books| 37|\n",
    "|Booming| 7|\n",
    "|Business Day| 100|\n",
    "|Corrections| 10|\n",
    "|Crosswords & Games| 2|\n",
    "|Dining & Wine| 19|\n",
    "|Education| 4|\n",
    "|Fashion & Style| 46|\n",
    "|Great Homes and Destinations| 5|\n",
    "|Health| 10|\n",
    "|Home & Garden| 10|\n",
    "|Magazine| 11|\n",
    "|Movies| 28|\n",
    "|N.Y. / Region| 92|\n",
    "|Opinion| 84|\n",
    "|Paid Death Notices| 11|\n",
    "|Real Estate| 13|\n",
    "|Science| 18|\n",
    "|Sports| 134|\n",
    "|Technology| 13|\n",
    "|Theater| 16|\n",
    "|Travel| 9|\n",
    "|U.S.| 88|\n",
    "|World | 131|\n",
    "|Your Money | 6 |\n",
    "\n",
    "1. Use pandas' `pd.read_pickle()`. to load data to DataFrame. Apply kmeans clustering to the `articles.pkl`.\n",
    "\n",
    "2. To find out what \"topics\" Kmeans has discovered we must inspect the centroids. Print out the centroids of the Kmeans clustering.\n",
    "\n",
    "   These centroids are simply a bunch of vectors.  To make any sense of them we need to map these vectors back into our 'word space'.  Think of each feature/dimension of the centroid vector as representing the \"average\" article or the average occurances of words for that cluster.\n",
    "\n",
    "3. But for topics we are only really interested in the most present words, i.e. features/dimensions with the greatest representation in the centroid.  Print out the top ten words for each centroid.\n",
    "\n",
    "    * Sort each centroid vector to find the top 10 features\n",
    "    * Go back to your vectorizer object to find out what words each of these features corresponds to.\n",
    "\n",
    "4. Look at the docs for `TfidfVectorizer` and see if you can limit the number of features (words) included in the feature matrix.  This can help reduce some noise and make the centroids slightly more sensible.  Limit the `max_features` and see if the words of the topics change at all.\n",
    "\n",
    "5. An alternative to finding out what each cluster represents is to look at the articles that are assigned to it.  Print out the titles of a random sample of the articles assigned to each cluster to get a sense of the topic.\n",
    "\n",
    "6. What 'topics' has kmeans discovered? Can you try to assign a name to each?  Do the topics change as you change k (just try this for a few different values of k)?\n",
    "\n",
    "7. If you set k == to the number of NYT sections in the dataset, does it return topics that map to a section?  Why or why not?\n",
    "\n",
    "8. Try your clustering only with a subset of the original sections.  Do the topics change or get more specific if you only use 3 sections (i.e. Sports, Art, and Business)?  Are there any cross section topics (i.e. a Sports article that talks about the economics of a baseball team) you can find? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data from articles.pkl to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "articles_df = pd.read_pickle(\"data/articles.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "articles_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vectorize the article content as tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(articles_df['content'])\n",
    "features = vectorizer.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply k-means clustering to the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans()\n",
    "\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"cluster centers:\")\n",
    "print(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the top 10 features for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_centroids = kmeans.cluster_centers_.argsort()[:,-1:-11:-1]\n",
    "print(\"top features for each cluster:\")\n",
    "for num, centroid in enumerate(top_centroids):\n",
    "    print(\"%d: %s\" % (num, \", \".join(features[i] for i in centroid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limit the number of features and see if the words of the topics change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "X = vectorizer.fit_transform(articles_df['content'])\n",
    "features = vectorizer.get_feature_names()\n",
    "kmeans = KMeans()\n",
    "kmeans.fit(X)\n",
    "top_centroids = kmeans.cluster_centers_.argsort()[:,-1:-11:-1]\n",
    "print(\"top features for each cluster with 1000 max features:\")\n",
    "for num, centroid in enumerate(top_centroids):\n",
    "    print(\"%d: %s\" % (num, \", \".join(features[i] for i in centroid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print out the titles of a random sample of the articles assigned to each cluster to get a sense of the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assigned_cluster = kmeans.transform(X).argmin(axis=1)\n",
    "\n",
    "# assigned_cluster = kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(kmeans.n_clusters):\n",
    "    cluster = np.arange(0, X.shape[0])[assigned_cluster==i]\n",
    "    sample_articles = np.random.choice(cluster, 3, replace=False)\n",
    "    print(\"cluster %d:\" % i)\n",
    "    for article in sample_articles:\n",
    "        print(\"    %s\" % articles_df.loc[article]['headline'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you set `k==` to the number of NYT sections in the dataset, does it return topics that map to a section?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=10)\n",
    "kmeans.fit(X)\n",
    "assigned_cluster = kmeans.transform(X).argmin(axis=1)\n",
    "print(\"top 2 topics for each cluster\")\n",
    "for i in range(kmeans.n_clusters):\n",
    "    cluster = np.arange(0, X.shape[0])[assigned_cluster==i]\n",
    "    topics = articles_df.loc[cluster].dropna()['section_name']\n",
    "    most_common = Counter(topics).most_common()\n",
    "    if len(most_common) > 1:\n",
    "        print(\"Cluster %d: %s\" % (i, most_common[0][0]),\", %s\" % (most_common[1][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try clustering with a subset of the sections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create masks\n",
    "cond_sports = articles_df['section_name']=='Sports'\n",
    "cond_arts = articles_df['section_name']=='Arts'\n",
    "cond_business_day = articles_df['section_name']=='Business Day'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "three_articles_df = articles_df[cond_sports | cond_arts | cond_business_day]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "three_articles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3)\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(three_articles_df['content'])\n",
    "kmeans.fit(X)\n",
    "assigned_cluster = kmeans.transform(X).argmin(axis=1)\n",
    "print(\"top 2 topics for each cluster\")\n",
    "for i in range(kmeans.n_clusters):\n",
    "    cluster = np.arange(0, X.shape[0])[assigned_cluster==i]\n",
    "    topics = three_articles_df.loc[cluster].dropna()['section_name']\n",
    "    most_common = Counter(topics).most_common()\n",
    "    print(\"Cluster %d: %s\" % (i, most_common[0][0]))\n",
    "    if len(most_common) > 1:\n",
    "        print(\" %s\" % (most_common[1][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have been introduced to distance metrics and the idea of similarity, but we will take a deeper dive here. For many machine learning algorithms, the idea of 'distance' between two points is a crucial abstraction to perform analysis. For Kmeans we are usually limited to use Euclidean distance even though our domain might have a more approprite distance function (i.e. Cosine similarity for text).  With Hierarchical clustering we will not be limited in this way.   \n",
    "We already have our bags and played around with Kmeans clustering.  Now we are going to leverage [Scipy](http://www.scipy.org/) to perform [hierarchical clustering](http://en.wikipedia.org/wiki/Hierarchical_clustering).\n",
    "\n",
    "1. Hierarchical clustering is more computationally intensive than Kmeans.  Also it is hard to visualize the results of a hierarchical clustering if you have too much data (since it represents its clusters as a tree). Create a subset of the original articles by filtering the data set to contain at least one article from each section and at most around 100 total articles.\n",
    "\n",
    "    One issue with text (especially when visualzing/clustering) is high dimensionality.  Any method that uses distance metrics is susceptible to the [curse of dimensionality](http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/). `scikit-learn` has some utility to do some feature selection for us on our bags.  \n",
    "\n",
    "2. The first step to using `scipy's` Hierarchical clustering is to first find out how similar our vectors are to one another.  To do this we use the `pdist` [function](http://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.pdist.html) to compute a similarity matrix of our data (pairwise distances).  First we will just use Euclidean distance.  Examine the shape of what is returned.\n",
    "\n",
    "3. A quirk of `pdist` is that it returns one looong vector.  Use scipy's [squareform](http://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.squareform.html) function to get our long vector of distances back into a square matrix.  Look at the shape of this new matrix.\n",
    "\n",
    "4. Now that we have a square similarity matrix we can start to cluster!  Pass this matrix into scipy's `linkage` [function](http://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html) to compute our hierarchical clusters.\n",
    "\n",
    "5. We in theory have all the information about our clusters but it is basically impossible to interpret in a sensible manner.  Thankfully scipy also has a function to visualize this madness.  Using scipy's `dendrogram` [function](http://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.dendrogram.html) plot the linkages as a hierachical tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a subset of the original articles by filtering the data set to contain at least one article from each section and at most 100 total articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "small_mask = np.zeros(len(articles_df)).astype(bool)\n",
    "indices = np.arange(len(articles_df))\n",
    "for category in articles_df['section_name'].unique():\n",
    "    category_mask = (articles_df['section_name'] == category).values\n",
    "    new_index = np.random.choice(indices[category_mask])\n",
    "    small_mask[new_index] = True\n",
    "additional_indices = np.random.choice(indices[np.logical_not(small_mask)],\n",
    "                                      100 - sum(small_mask),\n",
    "                                      replace=False)\n",
    "small_mask[additional_indices] = True\n",
    "small_df = articles_df.loc[small_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Verify that this is good:\n",
    "assert len(small_df) == 100\n",
    "assert len(small_df['section_name'].unique()) == len(articles_df['section_name'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First vectorize our articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "small_X = vectorizer.fit_transform(small_df['content'])\n",
    "small_features = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before using scipy's Hierarchical clustering, we need to first find out how similar our vectors are to one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now get pairwise distances\n",
    "distxy = squareform(pdist(small_X.todense(), metric='cosine'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pass this matrix into scipy's linkage function to compute our hierarchical clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "link = linkage(distxy, method='complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using scipy's dendrogram function plot the linkages as a hierachical tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dendro = dendrogram(link, color_threshold=1.5, leaf_font_size=9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To make your clusters more interpretable, change the labels on the data to be the titles of the articles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dendro = dendrogram(link, color_threshold=1.5, leaf_font_size=9,\n",
    "                    labels=small_df['headline'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label each point with the title and the section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(12, 6))\n",
    "\n",
    "labels = (small_df['headline'] + ' :: ' + small_df['section_name']).values\n",
    "dendro = dendrogram(link, color_threshold=1.5, leaf_font_size=9,\n",
    "                    labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(12, 36))\n",
    "\n",
    "labels = (small_df['headline'] + ' :: ' + small_df['section_name']).values\n",
    "dendro = dendrogram(link, color_threshold=1.5, leaf_font_size=9,\n",
    "                    labels=labels, orientation='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Form flat clusters from linakge matrix by setting threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# form clusters from linkage matrix by setting threshold\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "clusters = fcluster(link, t=1.14)\n",
    "\n",
    "df_res = pd.DataFrame({'section_name':small_df['section_name'],'clusters':clusters})\n",
    "#print df_res\n",
    "df_res['count'] = 1\n",
    "print(df_res[['section_name','count']].groupby(['section_name']).sum())\n",
    "print(df_res[['clusters','count']].groupby(['clusters']).sum())\n",
    "print(df_res[['clusters','section_name','count']].groupby(['clusters','section_name']).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explore different clusters on a per section basis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_dendrogram_by_categorty(articles_df, category, n_articles=20):\n",
    "    mask = articles_df['section_name'] == category\n",
    "    cat_df = articles_df[mask].sample(n=n_articles)\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "    cat_X = vectorizer.fit_transform(cat_df['content'])\n",
    "    distxy = squareform(pdist(cat_X.todense(), metric='cosine'))\n",
    "    fig, ax = plt.subplots(1, figsize=(6, 6))\n",
    "    \n",
    "    labels = cat_df['headline'].values\n",
    "    # labels = (cat_df['headline'] + ' :: ' + cat_df['subsection_name']).values\n",
    "    \n",
    "    dendro = dendrogram(linkage(distxy, method='complete'),\n",
    "                        color_threshold=4,\n",
    "                        leaf_font_size=8,\n",
    "                        labels=labels,\n",
    "                        orientation='right')\n",
    "    ax.set_title(category)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for category in ['Arts', 'Sports', 'World']:\n",
    "    plot_dendrogram_by_categorty(articles_df, category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform the same analysis as above and inspect the dendrogram with the words from the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 120))\n",
    "distxy_words = squareform(pdist(small_X.T.todense(), metric='cosine'))\n",
    "dendro = dendrogram(linkage(distxy_words, method='complete'),\n",
    "                    color_threshold=2, leaf_font_size=8,\n",
    "                    labels=small_features, orientation='right')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
